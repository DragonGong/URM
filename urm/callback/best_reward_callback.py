import os
from stable_baselines3.common.callbacks import BaseCallback
from collections import deque


class BestRewardCallback(BaseCallback):
    def __init__(self, save_path: str, window_size: int = 100, verbose=0):
        super().__init__(verbose)
        self.save_path = save_path
        self.window_size = window_size
        self.reward_buffer = deque(maxlen=window_size)
        self.best_mean_reward = -float('inf')  # åˆå§‹åŒ–ä¸ºè´Ÿæ— ç©·

    def _on_step(self) -> bool:
        dones = self.locals.get("dones", [])
        infos = self.locals.get("infos", [])

        for done, info in zip(dones, infos):
            if done:
                # ä» Monitor æ³¨å…¥çš„ info ä¸­è·å– episode æ€» reward
                if "episode" in info:
                    episode_reward = info["episode"]["r"]
                    self.reward_buffer.append(episode_reward)

                    # è®¡ç®—æ»‘åŠ¨çª—å£å¹³å‡ reward
                    current_mean_reward = sum(self.reward_buffer) / len(self.reward_buffer)

                    # å¦‚æœæ¯”å†å² best æ›´å¥½ï¼Œä¿å­˜æ¨¡å‹
                    if current_mean_reward > self.best_mean_reward:
                        self.best_mean_reward = current_mean_reward
                        # ä¿å­˜æ¨¡å‹ï¼ˆæ³¨æ„ï¼šè·¯å¾„éœ€åŒ…å«æ–‡ä»¶åï¼Œä¸å« .zipï¼‰
                        self.model.save(self.save_path)
                        if self.verbose > 0:
                            print(
                                f"ğŸ‰ New best mean reward: {current_mean_reward:.2f} - Model saved to {self.save_path}")

                    self.logger.record("train/best_mean_reward", self.best_mean_reward)
                    self.logger.record("train/current_mean_reward", current_mean_reward)

        return True
